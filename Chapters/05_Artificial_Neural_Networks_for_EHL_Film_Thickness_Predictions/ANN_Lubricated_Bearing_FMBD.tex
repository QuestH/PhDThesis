\chapter{Artificial Neural Networks for EHL Film Thickness Predictions}
\label{ANN Lubricated Bearing FMBD}

\section{Introduction}

Tribodynamic modelling generally employs analytical equations for the prediction of film thickness in elastohydrodynamic contacts; chosen due to their timely solution. Whilst computationally efficient, these do not achieve the accuracy of the full numerical solution outside the bounds of the data used to generate the analytical equations. In the context of dynamic simulation, a full numerical solution at each time step of a system level model would, however, yield excessive computation time. This has led to the emerging use of data driven solutions, such as machine learning, in the field of tribology. These can achieve accuracy much closer to the numerical solution, whilst significantly improving computational time.

This chapter details the development of an Artificial Neural Network (ANN) for prediction of central film thickness at the roller-race conjunction. ANNs are trained using data generated by the numerical EHL solution, with the data set constrained to realistic operating conditions using the Greenwood regimes of lubrication. Multiple ANNs are compared to find the optimum structure, accounting for training time and accuracy. The ANN is then deployed explicitly, using the boundary conditions of a simple bearing model to test the film thickness accuracy and speed of solution. The trained ANN is then deployed implicitly in the system level FMBD introduced in Chapter \ref{Lubricated FMBD}, replacing the analytical film equations in the model.

The aim of this chapter is to improve the accuracy of the central film thickness estimation, whilst maintaining a timely solution in the context of a full dynamic solution. This workflow employed is not only relevant to roller bearings; it can be applied to a wide range of contacts and different sources of training data depending on the modelling requirements.

\section{Numerical vs Analytical Film Thickness Estimations at High Entrainment Velocities} 
\label{Numerical vs Analytical Film Thickness Estimations at High Entrainment Velocities}

Two main approaches exist for determination of the complex non-linear problem of film thickness in lubricated contacts. The first approach involves employing numerical methods \cite{Dowson1959}, where systems of partial differential equations are formulated to describe the state of the contact and then solved iteratively \cite{Gohar2018}. Whilst this method yields accurate results and is applicable to a wide range of operating conditions, it is computationally intensive due to its iterative nature. The second approach involves developing regressed analytical equations from experimental or numerical studies which can be used for specific lubrication regimes. These equations offer quick estimates of key parameters, such as central \cite{Dowson1979} and minimum film thickness \cite{Dowson1967}. However, whilst more computationally efficient than the full numerical solution, this approach has limitations.
 
The applicability of regressed equations is often limited to the range of data used for their development. There is also a requirement for extensive effort in collecting experimental or numerical data to develop them. The entrainment velocities considered in this work (up to ~32~$m \cdot \mathrm{s}^{-1}$) exceed the typical range over which the regressed equations are experimentally derived. Whilst it is possible to exceed the range of input data, it must be done with caution \cite{Gohar1988}.

Figure \ref{EHL_NumericalvsAnalytical_25000} shows a comparison between the central film thickness calculated using the numerical method (see Section \ref{1D EHL Model}), and the analytical equation (Equation \ref{dimensionless central film thickness}) across a speed range of 1~000~-~21~000~$rpm$. The bearing used for this comparison is the same as in Section \ref{System level flexible model}. Geometry is detailed in Table \ref{Cylindrical Roller Bearing Specification}, with rheological and material properties detailed in Table \ref{Bearing Rheological Properties}.

\begin{figure}
	\centering
	\includegraphics[width=140mm]{AnalyticalvsNumerical_PercentageDiff.png}
	\caption{Central film thickness - EHL vs analytical.}
	\label{EHL_NumericalvsAnalytical_25000}
\end{figure} 

It is shown that as rotational speed and hence entrainment velocity increase, the film thickness prediction of the numerical and analytical calculations diverge. At 21~000~$rpm$, entrainment velocities of 30.7~$m \cdot \mathrm{s}^{-1}$ leads to a 20.3\% difference between the methods, with the analytical equation overestimating the film thickness.

The implementation of ANNs within tribology is one way to overcome the computational expense of the full numerical solution and this limited validity of the analytical approach. Overcoming the above stated discrepancy is not the only advantage of a numerical EHL informed ANN. Since an ANN can be trained using a wide range of input data, the effects of other tribological phenomena such as starvation and thermal effects can also be considered. Furthermore, the training of such ANNs is not bound to only numerical inputs; the results of experimental testing could be used to generate an experimentally validated ANN.

\section{ANN Fundamentals}

An Artifical Neural Network (ANN) is a computational model that is inspired by the biological neural networks present in the natural brain \cite{Sarkar2017}. ANNs are a subset of machine learning (ML) that can be trained using supervised, unsupervised, or reinforcement learning techniques. In supervised learning, ANNs are particularly useful for regression tasks, where they can model complex non-linear relationships between inputs and outputs. ANNs compare their outputs with target values during training and, due to their structure, can adapted for a wide range of applications. The goal of this training is to minimise the error, and to improve the ability of the network to generalise and make accurate predictions for new, unseen data.

ANNs consist of a set of interconnected processing elements known as neurons. These are represented computationally as nodes, and the terms are often used interchangeably. These elements have the ability to adapt to input data for the purpose of solving complex non-linear functions. The neurons are organised into three main layers, shown in Figure \ref{ANN schematic}: Input layer; Hidden layer(s) and Output layer. The adaptation is performed using weighted connections that link each neuron layer. These weightings are adjusted during the learning process, and determine the strength of the connections between neurons.

\begin{figure}
	\centering  
	\includegraphics[width=150mm]{ANN_schematic.png}
	\caption{ANN schematic.}
	\label{ANN schematic}
\end{figure} 

The following section provides descriptions of the common terms that will be referenced throughout this chapter.

\subsection{Network Architecture}
\begin{itemize}
	\item \textbf{Input layer}: Input data is assigned to input nodes in the first layer of the network. In the case of the film thickness estimation, nine input variables and hence nine nodes are required.
	\item \textbf{Hidden layer(s)}: These layers are located between the input and output layer. There can be multiple hidden layers, each consisting of multiple nodes. Multi-layer networks enable the resolution of non-linear problems, whereas single layer networks (no hidden layers) are limited to linear problems \cite{Bell2014}.
	\item \textbf{Output layer}: The output layer represents the final computed results. For the film thickness estimation, the output consists of a single node corresponding to the predicted central film thickness.
\end{itemize}

The type of ANN used in this study is called a multi-layer feedforward backpropogation neural network. Forward propogation through the network is used to make predictions based on the input data. Backpropogation is then performed to calculate prediction error and modify the network to minimise this. This process is repeated until the desired prediction accuracy is achieved.

\subsection{Forward Propogation}

Input data propagates through the neural network in the following manner:
\begin{enumerate}
	\item \textbf{Weights}: The input data to the nodes is multiplied by corresponding weights, and a bias term is added (see Figure \ref{ANN schematic}). The general equation for ANNs is:
	
	\begin{equation}\label{Generalised ANN1}
		z = f(b+\sum_i W_{j i}^{(n)} X_i^{(n)})
	\end{equation}
	
	where $z$ represents the activated output of the neuron. $W_{j i}$ is the weight connecting the $i$-th neuron of the previous layer to the $j$-th neuron of the current layer in the $n$-th layer. $X_i^{(n)}$ is the input value to the neuron from the $i$-th neuron in the previous layer. The bias is represented by $b$, and $f$ represents the activation function applied to the weighted sum.
	
	\item \textbf{Activation function}: The weighted sum plus bias ($x_w = b+\sum_i W_{j i}^{(n)} X_i^{(n)}$) is passed through an activation function, $f$, which introduces non-linearity to the system to enable the learning of complex patterns.
	
	\item \textbf{Bias}: The bias term, $b$, is able to shift the activation function's output. It ensures that a neuron can still activate even in the case where all input values are zero.
	
	\item \textbf{Output generation}: Data is propagated through the system until it reaches the final prediction at the output layer.
\end{enumerate}

\subsection{Backpropagation}

The learning process of an ANN is achieved by adjusting the weights in the network. This is done using a process called backpropagation \cite{Bishop2006}:
\begin{enumerate}
	\item \textbf{Loss function}: During training, the target output of the ANN is known. The predicted output of the ANN is therefore measured against this using a loss function. In the case of ANNs used for regression, mean squared error (MSE) is used.
	\item \textbf{Backpropogation}: This error is backpropogated through the system to determine the contribution of each weight to that error.
	\item \textbf{Optimization}: Optimization algorithms, such as Levenberg-Marquardt \cite{Levenberg1944} \cite{Marquardt1963} \cite{Wilamowski2010}, are used to update the weights and biases to reduce the loss.
	\item \textbf{Epochs}: The above process repeats over multiple cycles, known as epochs. This is performed until the desired MSE is achieved.
	\item \textbf{Overfitting}:  This is the phenomenon whereby an ANN becomes too specialised at learning the training data, and as a result performs poorly with new, unseen data. This occurs when the network extensively adjusts its internal parameters to fit noise or outliers in the training set \cite{Ying2019}. It is therefore necessary to limit the number of epochs once sufficient performance is achieved.
\end{enumerate}

\section{Methodology}

This section details the following methodology:

\begin{enumerate}
	\item Generating training data for the ANN using the numerical EHL method, and constraining this input data to a range valid for machine element contacts.
	\item Evaluating the best ANN structure for the central film thickness estimation.
	\item Testing the ANN using by calculating bearing film thickness explicitly based on kinematic condition obtained from a dynamic bearing model.
	\item Embedding the ANN within a FMBD model to calculate and implicitly consider the film thickness within the bearing at each time step of the simulation.
	
\end{enumerate}

The workflow describing the EHL data generation, variable constraints, training methodology and structure evaluation is presented in Figure \ref{ANN flowchart}. This workflow resulted in a structurally optimised trained ANN that could be used for the explicit and implicit modelling tests.

\begin{figure}  
	\centering
	\includegraphics[width=105mm]{ANN_structure_flowchart.png}
	\caption{ANN data flow, models and training flowchart.}
	\label{ANN flowchart}
\end{figure} 

\subsection{EHL Input Data Generation}

Training an ANN requires a comprehensive data set. For this study, the training data was generated using the numerical EHL model presented in \ref{1D EHL Model}. This approach was selected due to the large size of the dataset required for training, and the relatively low resource-intensive nature to generate this. It is important to note that the training data could also be obtained from experimental work, which would further enhance the applicability of this approach for future studies.

\subsubsection{Sampling Input Variables}

Due to the large design space covered by the high number and range of input parameters, a robust sampling technique must be chosen to create the training data set. In traditional random sampling, each of the parameters is randomly sampled within its defined range. This may lead to insufficient coverage of the parameter space and simple bias, as it lacks a systematic approach to ensure even distribution \cite{Preece2016}.

The Latin Hypercube Sampling (LHS) method was utilised by Marian et al. \cite{Marian2022}, and was also selected for this study. It is a statistical method used to efficiently sample a high-dimensional parameter space, such as that required for the central film thickness calculation. LHS is derived from Latin Hypercube Design (LHD), where each parameter’s range is divided into equal intervals along each dimension. Each interval is then randomly assigned to a unique position within its corresponding dimension. The process results in a matrix, where each row represents a combination of parameter values. Contrary to the random sampling method, LHS ensures that each interval is sampled exactly once per dimension, preventing clustering and improving representation across the space \cite{Preece2016}. This ensures lower computational effort required for ANN training, despite the high number of input variables and value ranges.

The LHD is a $n_{\mathrm{s}} \times n_{\mathrm{f}}$ matrix, where $n_s$ and $n_f$ represent the number of simulations the number of factors respectively. LHS enhances LHD by introducing a randomization component. The randomly selected samples within each interval undergo permutation, ensuring that samples are not biased by the order of selection.

LHS elements are generated by subtracting a random number between zero and one $Z_{\mathrm{r}}\in[0,1]$ from each LHD element $x_{i j, \mathrm{LHD}}$. This is then divided by the number of test points \cite{Siebertz2010}:

\begin{equation}\label{LHS}
	x_{i j, \mathrm{LHS}}=\frac{x_{i j, \mathrm{LHD}}-Z_{\mathrm{r}}}{n_{\mathrm{s}}}
\end{equation}

This equation rescales the LHD values to a range between 0 and 1. By subtracting a random number between 0 and 1 and dividing by the total number of sample points, the resulting Latin hypercube samples are spread evenly across the interval (0,1) for each parameter. This is important, because it allows the Latin hypercube samples to be easily transformed to any desired range or distribution. This transformation to the design space is done using the limits of the tribological parameters in Table \ref{Sensitivity study of ANN structure}.

The quality of the test field (freedom of correlation and uniform distribution) can be assessed based on the distances between data points \cite{Johnson1990}. The MaxiMin criterion in the MATLAB\textregistered\ Statistics and Machine Learning toolbox was used to optimise the LHS. This maximises the the minimum distance between individual test points such that the LHS test field is uniformly distributed:

\begin{equation}\label{maximin}
	\operatorname{MaxiMin}=\left[\sum_{1 \leq i<j \leq n_1} d\left(x_i, x_j\right)^{-\xi}\right]^{-\frac{1}{\xi}}
\end{equation}

where $d$ represents all distances in the test field, and subscripts $i$ and $j$ are indexes for the parameter and sample point respectively. $\xi$ represents the application dependant factor which determines the degree of importance assigned to the distances \cite{Siebertz2010}.

\subsubsection{Constraining the Input Data Bounds}

The performance of ANNs is heavily reliant upon the quality of the data set provided for training. To construct a training database, Marian et al. \cite{Marian2022} utilised a Finite Element Method (FEM) solver for film thickness calculations. The database covered a very large range of lubricant and material properties for relatively low entrainment speed conditions (< 0.4~$m \cdot \mathrm{s}^{-1}$ for the 2D line contact studies). Contact conditions for some combinations of these input parameters exceed realistic conditions within common machine elements, including bearings. To further improve upon this methodology, the input data range required constraining.

The Greenwood Regime chart \cite{Johnson1970} was used for this purpose. The regions of the chart, as shown in Figure \ref{Greenwood informed training data vs Marian et al.}, are:

\begin{figure}
	\centering  
	\includegraphics[width=140mm]{ANN_Explicit_MarianvsGreenwood.png}
	\caption[Greenwood informed training data vs Marian et al.]{Greenwood informed training data vs Marian et al. \cite{Marian2022}.}
	\label{Greenwood informed training data vs Marian et al.}
\end{figure} 

\begin{itemize}
	\item Isoviscous Rigid (IR)
	\item Isoviscous Elastic (IE)
	\item Piezoviscous Rigid (PR)
	\item Piezoviscous Elastic (PE)
\end{itemize}

The bounds indicate the transition between the lubrication regimes, which are classified based on material, rheological and geometric properties. To find which regime a contact is operating within, the dimensionless elasticity ($G_e$) and viscosity ($G_v$) parameters can be calculated:

\begin{equation}\label{G_e}
	G_e=\left(\frac{\alpha^2 W_i^3}{\eta_0 u R_r^2}\right)^{\frac{1}{2}}
\end{equation}

\begin{equation}\label{G_v}
	G_v=\left(\frac{W_i^2}{\eta_0 u E_r R_r}\right)^{\frac{1}{2}}
\end{equation}

The PE region signifies an EHL contact, where pressures are high enough to elastically deform the material and increase the viscosity of the lubricant. The IR region relates to the hydrodynamic regime of lubrication, where the contact load does not deform the surface and viscosity remains constant. Since these investigations are focussed on improving the EHL film thickness estimation, the training data set was required to fall within the PE region of the Greenwood plot. 

The initial range of each parameter is shown in Table \ref{Range of ANN film thickness calculation parameters}. A training data set was then generated using these limits. The input data was then constrained further to ensure Hertzian pressures, $P$, were between 300~$MPa$ and 3.5~$GPa$, as well as redistributing any points that fell outside of the PE and PR regions. A flowchart to explain the process of constraining the input variables is shown in Figure \ref{EHL_Greenwood_Constraints_Flowchart}.

\begin{figure}
	\centering  
	\includegraphics[width=130mm]{EHL_Greenwood_Constraints_Flowchart.png}
	\caption{Workflow to constrain training input data using Greenwood regimes.}
	\label{EHL_Greenwood_Constraints_Flowchart}
\end{figure} 



\begin{table*}
	%\captionsetup{justification=centering}
	\caption{Range of ANN film thickness calculation parameters}
	\label{Range of ANN film thickness calculation parameters}
	\centering
	\renewcommand{\arraystretch}{1.5}%
	\begin{tabular}{|P{0.4\textwidth}|P{0.15\textwidth}|P{0.15\textwidth}|P{0.15\textwidth}|}
		\hline
		\textbf{Parameter} & \textbf{Unit} & \textbf{Minimum} & \textbf{Maximum} \\ [0.5ex]
		\hline
		Load & $N$ & 150 & 5000 \\ [0.5ex]
		\hline
		Entrainment velocity & $m \cdot \mathrm{s}^{-1}$ & 0.6 & 30 \\ [0.5ex]
		\hline
		Reduced radius & $m$ & 0.0001 & 0.02 \\ [0.5ex]
		\hline
		Reduced elastic modulus & $GPa$ & 200 & 250 \\ [0.5ex]
		\hline
		Pressure-viscosity coefficient & ${GPa}^{-1}$ & 10 & 30 \\ [0.5ex]
		\hline
		Lubricant viscosity & $Pa \cdot s$ & 0.0005 & 0.1 \\ [0.5ex]
		\hline
		Lubricant density & ${kg}/{m}^3$ & 7750 & 8050 \\ [0.5ex]
		\hline
		Poisson's ratio & $-$ & 0.3 & 0.35 \\ [0.5ex]
		\hline
		Contact length & $m$ & 0.001 & 0.050 \\ [0.5ex]
		\hline
		
	\end{tabular}
\end{table*}

A comparison of an the unconstrained and constrained input variables used for the training data is shown in Figure \ref{Training_Unconstrained_GreenwoodRegime}. It is shown that for the same number of data points (5~000), the constrained data cloud is concentrated over a smaller region of the chart. This improved the point density in regions of interest, increasing the likelihood that the training data more closely matches the test data.

\begin{figure}
	\centering  
	\includegraphics[width=140mm]{Training_Unconstrained_GreenwoodRegime.png}
	\caption{Greenwood informed training data vs unconstrained data.}
	\label{Training_Unconstrained_GreenwoodRegime}
\end{figure} 

\subsubsection{Constructing the Numerical Database}

The 1D EHL model presented in Section \ref{1D EHL Model} was used to generate the numerical database for training the ANN. The variables corresponding to each constrained data point in Figure \ref{Training_Unconstrained_GreenwoodRegime} were used as inputs to the calculation. The target output of central film thickness was calculated.

\subsection{ANN Structure Evaluation} \label{ANN Structure Evaluation}

The general structure of the ANN is described in the following format, as per \cite{Zhang2002}:

\begin{equation}
	N_{i n}-\left[N_{h 1}-N_{h 2}-N_{h 3}\right]_t-N_{\text {out }}
\end{equation}

The number of neurons in each layer is denoted by $N$, with the input and output layers indicated by the subscripts $in$ and $out$, respectively. Subscripts $h1$, $h2$, and $h3$ denote the hidden layers, with $t$ being the total number of hidden layers. A graphic representation of the structure used for the film thickness estimations is show in Figure \ref{ANN structure}. As the structural complexity of ANNs increases, the training time increases due to the greater number of neurons and layers. Implementations of ANN in the field of tribology, specifically film thickness predictions, are typically limited to between one and three hidden layers \cite{Marian2021}.

\begin{figure}
	\centering  
	\includegraphics[width=150mm]{ANN_Structure.png}
	\caption{ANN structure to predict EHL central film thickness ($9-[14-14-14]_3-1$).}
	\label{ANN structure}
\end{figure} 

The structure of an ANN affects both its training time and prediction accuracy. To evaluate the performance of different ANN structures, and hence select an appropriate structure for this application, a sensitivity study was performed. The study comprised of over 500 different ANN structures. The input data range remained constant across all structures, whilst the variables listed in Table \ref{Sensitivity study of ANN structure} were adjusted. This involved varying the hyperparameters: the number of hidden layers varied from one to four, and the number of neurons from 10 to 20. Three activation functions: Hyperbolic tangent, Logistic sigmoid and Rectilinear were evaluated. The wall time for each training data point generation was recorded, as well as the total training time of each ANN structure.

Selection of the final structure to be used for film thickness estimation in the bearing models was based on total training time, coefficient of determination ($R^2$), and the potential for the ANN to overfit. $R^2$ (Equation \ref{R-squared}) was evaluated for the test data sets, since this gives an accurate indication of the networks ability to predict film thickness with unseen input variables.

\begin{table*}
	%\captionsetup{justification=centering}
	\caption{Sensitivity study of ANN structure}
	\label{Sensitivity study of ANN structure}
	\centering
	\renewcommand{\arraystretch}{1.5}%
	\begin{tabular}{|P{0.4\textwidth}|P{0.4\textwidth}|}
		\hline
		\textbf{Variable} & \textbf{Value} \\ [0.5ex]
		\hline
		Number of training data points & 600 - 1500 \\ [0.5ex]
		\hline
		Number of hidden layers, t & 1 - 4 \\ [0.5ex]
		\hline
		Number of neurons, N & 10 - 20 \\ [0.5ex]
	    \hline
		Activation function type & Hyperbolic tangent (Tanh), Logistic sigmoid (LogSig), Rectilinear (ReLU) \\ [0.5ex]
		\hline
	\end{tabular}
\end{table*}

The following sections describe the evaluation process of each network structure.

\subsubsection{Training, Validation and Test Datasets}

The dataset was first divided into three sets: the training set, the validation set, and the test set, each containing 70~$\%$, 15~$\%$ and 15~$\%$ of the training data respectively:

\begin{enumerate}
	\item \textbf{Training Set}: The training set is the portion of the dataset used to train the ANN, containing the input data and the corresponding output data. As aforementioned, the ANN adjusts the internal parameters based on this data to learn the underlying patterns.
	\item \textbf{Validation Set}: The validation set is used to tune the performance of the ANN during the training process. It is an independent dataset that the network has not seen before, allowing for the evaluation of its generalization capabilities. The network's performance on the validation set is monitored during training to make decisions on adjusting hyperparameters (number of hidden layers, neurons per hidden layer, activation functions), or stopping the training process to prevent overfitting.
	\item \textbf{Test Set}: The test set is a completely independent dataset that is not used during training or validation. It is used to assess the final performance and generalization ability of the trained ANN. By evaluating the network on unseen data, the test set provides an unbiased estimate of the model's performance in actual use.
\end{enumerate}

To evaluate the optimum ANN structure, the size of the training data set was varied (600, 1000, 2000 and 5000) to observe the its effect on the quality of the predictions. A limit of 1000 epochs was also implemented, restricting the ANN to 1000 full iterations through the entire training set. This achieved low MSE whilst preventing overfitting (see Section \ref{Preventing overfitting}).

\subsubsection{Data Normalisation}

The input and target parameters were normalised using the min-max normalisation function:

\begin{equation}
	\tilde{x}=\frac{x-x_{\min }}{x_{\max }-x_{\min }}(u_n-l_n)+u_n
\end{equation}

where $u_n$, and $l_n$ represent the upper and lower normalised unit values of 1 and -1 respectively. The dimensional target input value is denoted by $x$, and the final normalised input or output parameter of the ANN is denoted by $\tilde{x}$. $x_{\max }$ and $x_{\min }$ are retained to dimensionalise the output variable after the prediction.

\subsubsection{Activation Functions} \label{Activation functions}

As suggested in \cite{Marian2022}, four suitable activation functions for the hidden layers were selected for testing. These are mathematical functions that are applied to the output of each neuron in a layer of the neural network. They introduce non-linearity which allows the network to learn complex input-output relationships. Activation functions help determine the output of a neuron based on the weighted sum of its inputs. A description of each function is provided below:

\begin{itemize}
	\item \textbf{Sigmoid (logistic)}: This function transforms the input values into a range between 0 and 1. It has continuously differentiable smooth S-shaped curve and is given by the following formula \cite{Han1995}:
	
	\begin{equation}\label{Logistic sigmoid}
		\log \operatorname{sig}(x_w)=\frac{1}{1+e^{-x_w}}
	\end{equation} 
	
	Sigmoid functions may suffer from the "vanishing gradient" problem where the partial derivative reaches zero \cite{Sharma2020}, leading to slower convergence during training.
	
	\item \textbf{ReLU (Rectified Linear Unit)}: This function outputs the input value directly if it is positive, and zero otherwise. The mathematical definition is:
	
	\begin{equation}\label{ReLU}
		\operatorname{ReLU}= \begin{cases}x_w, & x_w \geq 0 \\ 0, & x_w \leq 0\end{cases}
	\end{equation}
	
	The gradient is 1 when the neuron is activated, and zero when it is deactivated. This function is computationally efficient and addresses the vanishing gradient problem to an extent \cite{Sharma2020}.
	
	\item \textbf{Tanh (Hyperbolic Tangent)}: The hyperbolic tangent or tanh function is defined as:
	
	\begin{equation}\label{Hyperbolic tangent}
		\tanh (x_w)=\frac{2}{1+e^{-2 x_w}}-1
	\end{equation}
	
	The formulation and behaviour is very similar to sigmoid. It produces values which range from -1 to 1, having a centred mean around zero.
	
	\item \textbf{Linear}: A simple linear activation was used on the output.
	
\end{itemize}

\subsubsection{Evaluating the Network Performance}

During backpropagation of the ANN, the Mean Squared Error (MSE) was used to evaluate the network's performance:

\begin{equation}\label{MSE}
	M S E=\frac{1}{N_p} \sum_{i=1}^{N_p}\left(t_i-y_i\right)^2
\end{equation}

where $t_i$ and $y_i$ are the target and predicted value respectively. The total number of training points being trained, validated or tested is denoted by $N_p$. 

To assess the goodness of fit of the ANN, the statistical metric $R^2$, known as the coefficient of determination, was used. This measures the proportion of variance in the dependant variable (film thickness) that is predictable from the input variables (Table \ref{Range of ANN film thickness calculation parameters}) in the model. This value ranges from 0 to 1, with a higher value indicating the best fit of the model to the data. This was post-processed after training and is calculated as follows:

\begin{equation}\label{R-squared}
	R^2=1-\frac{\sum_{i=1}^{N_p}\left(t_i-y_i\right)^2}{\sum_{i=1}^{N_p}\left(y_i-\bar{y}\right)^2}
\end{equation}

 where $\overline{\mathrm{y}}$ is the mean of the target sample. The numerator of the fraction, \( \sum_{i=1}^{N_p}(t_i - y_i)^2 \), represents the sum of squared residuals, which quantifies the variation in the target variable that is not explained by the model. The denominator, \( \sum_{i=1}^{N_p}(y_i - \bar{y})^2 \), is the total sum of squares, which captures the total variation in the target variable \cite{Marian2022}.

\subsubsection{Preventing Overfitting} \label{Preventing overfitting}

Early stopping and regularisation was used to prevent statistical overfitting during training \cite{MatlabOverfit}. Early stopping halts the training process before the model reaches the maximum number of epochs. This is done by monitoring the performance (MSE (Equation \ref{MSE})) of the network against the validation set during training. Once the performance reaches a plateau, or begins to degrade, the training is stopped early. Regularisation adds additional constraints to the learning process. It modifies the performance criteria by accounting for the change in mean square of the network weights and biases (Mean Squared Weight (MSW)). This is calculated in Equation \ref{MSW}: 

\begin{equation}\label{MSW}
	M S W=\frac{1}{N} \sum_{j=1}^N W_j^2
\end{equation}

where $W_j$ is the individual weight value associated with the $j$-th neuron or connection in the network.

By applying an adjustment factor, denoted as $\gamma^{\prime}$, the weights and biases can be reduced during propagation (Eq. \ref{MSE adjusted}), thus mitigating the risk of overfitting and improving the network's generalization capability.

\begin{equation}\label{MSE adjusted}
	M S E_{r e g}=\gamma^{\prime} * M S W+\left(1-\gamma^{\prime}\right) * M S E
\end{equation}

\subsection{Explicit Bearing Film Thickness Predictions} \label{Explicit Bearing Film Thickness Predictions}
After identifying a suitable training data size and structure, an ANN was trained to estimate the EHL central film thickness. These results could then be compared to the analytical (Equation \ref{DowsonToyodaCentralFilm}) and numerical (Section \ref{1D EHL Model}) methods for calculating the film thickness under realistic bearing operating conditions.

The FMBD model used in Chapter \ref{Lubricated FMBD} was used for this study. The shaft was modelled as a rigid body, and loading in the radial direction was purely static to remove the influence of additional dynamic effects. The shaft was constrained to one rotational and two lateral degrees of freedom. Bearing specification, material and rheological properties are shown in Tables \ref{Cylindrical Roller Bearing Specification} and \ref{Bearing Rheological Properties}.

The bearing was modelled as dry, without the influence of the EHL film at the roller-race contacts. The kinematic and dynamic results necessary for the film thickness estimation were extracted from an individual roller at each time step of the simulation. These results include roller load, contact entrainment velocity, and reduced radius of the contact between the roller and inner-race.

The loading pattern is cyclic in nature as the roller enters and exits the most highly loaded region of the bearing, corresponding to the radial force vector applied to the inner race. Sufficient preload ensures constant contact between elements and raceways so that the regime does not deviate from EHL. The contact reduced radius (3.03~$mm$) and entrainment speed (20~$m \cdot \mathrm{s}^{-1}$) do not change throughout the orbit as they are a function of bearing geometry and constant operating speed. The normal contact load fluctuation is shown in Figure \ref{Contact Normal Load ANN}.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{Bearing_Input_Force_FMBD_20_1.15kN_contact_force_data.png}
	\caption{Normal contact load vs cage angular displacement.}
	\label{Contact Normal Load ANN}
\end{figure}

The operating conditions of the bearing were within the range of validity of the training data set. This is demonstrated in Figure \ref{Greenwood_Bearing} whereby the Greenwood parameters for the bearing operating points are calculated and overlayed on the training data cloud.

\begin{figure}
	\centering  
	\includegraphics[width=140mm]{Greenwood_Bearing.png}
	\caption{Bearing operating conditions vs Greenwood informed training data.}
	\label{Greenwood_Bearing}
\end{figure} 

The structure chosen for this study was based on the conclusions drawn in Section \ref{ANN Structure Evaluation Results}. A structure of 3 hidden layers with 14 neurons per layer was selected ($9-[14-14-14]_3-1$). Logistic sigmoid was selected as the activation function for the hidden layers. This structure is represented graphically in Figure \ref{ANN structure}.


\subsection{Implicit Bearing Film Thickness Predictions}

To assess the viability of using ANNs as an alternative to the regressed film thickness equations in FMBD modelling, the trained ANN was embedded within the system level model introduced in Section \ref{System level flexible model}. The extrapolated film equation (\ref{DowsonToyodaCentralFilm}) used within the lubricated bearing model in Section \ref{Contact mechanics FMBD} was replaced with the trained neural network function.

The operating points investigated were at speeds of 5~000~-~20~000~$rpm$ in increments of 5~000~$rpm$. The models were run for a total input shaft angular displacement of 50~$rad$. The first 10~$rad$ of shaft rotation cover the period of system convergence. During this period, and for a further period of 20~$rad$, the analytical formula for the central film thickness calculation were used. Once the system reached steady state operation, and suitable computational the film thickness estimation was then performed using the trained ANN. 

Simulations were performed at a fixed time step of 1$\times 10^{-6}$~$s$ to ensure convergence of the explicitly coupled models. The CPU time per time step was measured. Since the solver utilised an angle based simulation interval, the number of calculation steps varied from 95~493 to 23~874 for 5~000~$rpm$ and 20~000~$rpm$ respectively.

Computational timings per point were recorded to assess the viability of this solution.


\section{Results and Discussion}

The following data was obtained using consumer grade hardware with the following specifications: 
Intel® Core™ i7-9750H CPU 6 cores @ 2.60GHz, 32GB RAM; GPU: NVDIA GeForce GTX 1650. Identical hardware was used for both the full numerical and the ANN solutions to provide performance comparisons and assess the suitability of ANNs for film thickness calculations in FMBD solvers.

\subsection{ANN Structure Evaluation} \label{ANN Structure Evaluation Results}

The 1D EHL model presented in Section \ref{1D EHL Model} was used to generate the numerical database for training the ANN. Each numerical solution and hence training point took an average of 5.88~$s$ to compute. The construction of the entire database on a single core therefore has a wall time of between 58.8~$min$ and 489~$min$ for 600 and 5000 points respectively. This wall time is noted for baseline comparisons, and can be significantly improved if parallelisation across multiple cores is utilised.

Tables \ref{LogSig_table} - \ref{ReLU_table} present the $R^2$ values obtained from 600 training data points, considering different activation functions described in Section \ref{Activation functions}. The number of layers and neurons was varied for each activation function. Among the activation functions tested, the rectilinear (ReLU) function consistently underperformed when compared to the logistic sigmoid (LogSig) and hyperbolic tangent (Tanh) functions across all network structures.  Similar to the work of Marian et al. \cite{Marian2021}, the optimum layers was found to be between two and three, however the activation function was shown to be the dominating determinate of $R^2$ performance.

The LogSig activation function demonstrated the best $R^2$ performance, even for lower complexity structures. This was therefore selected for each hidden layer to assess the influence of training data quantity $R^2$ performance. Tables \ref{ANN_Explicit_LogSig_600_table} - \ref{ANN_Explicit_LogSig_5000_table} present the training times for networks with varying numbers of training points and structural configurations. The results indicate that training time increases with the number of layers and neurons in the network. However, training time exhibits a much stronger positive correlation with the number of data points rather than with the complexity of the network structure.

Figure \ref{$R^2$ performance of ANN structure} illustrates the influence of the number of training points on the $R^2$ value when using the LogSig activation function. The results indicate that 600 data points are sufficient to train an ANN to the accuracy required for the central film thickness prediction ($R^2$ = 0.99791). Increasing the dataset to 5~000 data points resulted in an $R^2$ value of 1, however this came with significant time cost for structures with higher complexity; reaching 600~$s$ for 4 layers of 20 neurons and a total of 1000 epochs. Combined with a data generation time of 489~$min$, this represents a computationally inefficient trade-off between accuracy and training cost. It is therefore concluded that 1000~-~2000 training data points are sufficient to achieve accurately trained neural networks for predicting the central film thickness whilst maintaining computational efficiency.

The results of this study guided the decision for the network structure to be used for the bearing models. A structure of 3 hidden layers with 14 neurons per layer was selected ($9-[14-14-14]_3-1$). Logistic sigmoid was selected as the activation function for the hidden layers. This was trained using 2000 data points.

\begin{table}
	\caption{$R^2$ performance of ANN structures using 600 data points and a LogSig activation function}
	\label{LogSig_table}
	\includegraphics[width=\linewidth]{ANN_Explicit_ActFun_LogSig.jpg}
\end{table}

\begin{table}
	\caption{$R^2$ performance of ANN structures using 600 data points and a Tanh activation function}
	\label{Tanh_table}
	\includegraphics[width=\linewidth]{ANN_Explicit_ActFun_Tanh.jpg}
\end{table}

\begin{table}
	\caption{$R^2$ performance of ANN structures using 600 data points and a Tanh activation function}
	\label{ReLU_table}
	\includegraphics[width=\linewidth]{ANN_Explicit_ActFun_ReLU.jpg}
\end{table}

\begin{table}
	\caption{Training time of ANN structures with LogSig activation function and 600 data points}
	\label{ANN_Explicit_LogSig_600_table}
	\includegraphics[width=\linewidth]{ANN_Explicit_LogSig_600.jpg}
\end{table}

\begin{table}
	\caption{Training time of ANN structures with LogSig activation function and 1000 data points}
	\label{ANN_Explicit_LogSig_1000_table}
	\includegraphics[width=\linewidth]{ANN_Explicit_LogSig_1000.jpg}
\end{table}

\begin{table}
	\caption{Training time of ANN structures with LogSig activation function and 2000 data points}
	\label{ANN_Explicit_LogSig_2000_table}
	\includegraphics[width=\linewidth]{ANN_Explicit_LogSig_2000.jpg}
\end{table}

\begin{table}
	\caption{Training time of ANN structures with LogSig activation function and 5000 data points}
	\label{ANN_Explicit_LogSig_5000_table}
	\includegraphics[width=\linewidth]{ANN_Explicit_LogSig_5000.jpg}
\end{table}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{ANN_Explicit_600_Rval.png}
		\caption{}
		\label{600_Rval}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{ANN_Explicit_1000_Rval.png}
		\caption{}
		\label{1000_Rval}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{ANN_Explicit_2000_Rval.png}
		\caption{}
		\label{2000_Rval}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{ANN_Explicit_5000_Rval.png}
		\caption{}
		\label{5000_Rval}
	\end{subfigure}
	\caption{$R^2$ performance of ANN structure ($N$=14, $t$=3) using a Logistic Sigmoid activation function at each hidden layer: a) 600 points, b) 1000 points, c) 2000 points, d) 5000 points.}
	\label{$R^2$ performance of ANN structure}
\end{figure}


\subsection{Explicit Bearing Film Thickness Predictions}

The performance of the selected ANN was compared the to the computed numerical and analytical results during several operating cycles of the bearing model presented in Section \ref{Explicit Bearing Film Thickness Predictions}.

Figure \ref{ANN Film Thickness Comparisons} demonstrates that the ANN prediction of film thickness matches very closely with the numerical calculation. The periodic fluctuations arise due to the varying contact load as the roller enters and exits the loaded region. At minimum contact load, the maximum central film thickness is 2.410~$\mu \mathrm{m}$ and 2.396~$\mu \mathrm{m}$ for the ANN and numerical model respectively. By comparison, the analytical equation overestimates the peak film thickness to be 3.321~$\mu \mathrm{m}$.

\begin{figure}
	\centering
	\includegraphics[width=150mm]{Bearing_Test_Data_FMBD_20_1.15kN.png}
	\caption{ANN, numerical and analytical central film thickness comparisons.}
	\label{ANN Film Thickness Comparisons}
\end{figure}

Table \ref{Explicit time and MSE} demonstrates the relative performance of the ANN for both computation time and MSE for the analysis in Figure \ref{ANN Film Thickness Comparisons}. The ANN demonstrates a $\sim$1~570 factor computation time reduction in comparison with the ANN, whilst maintaining excellent accuracy as shown by the MSE. The analytical solution is a factor of $\sim$70 faster than the ANN, but is significantly less accurate. This achieved an MSE across the operating cycle of 8.47 $\times 10^{-1}$~$\mu \mathrm{m}^2$, compared to he ANN performance of 7.65 $\times 10^{-5}$~$\mu \mathrm{m}^2$. As discussed in Section \ref{Numerical vs Analytical Film Thickness Estimations at High Entrainment Velocities}, the analytical error will increase further with entrainment velocity.

\begin{table*}
	\caption{Film thickness computation methodology performance relative to the numerical solution}
	\label{Explicit time and MSE}
	\centering
	\renewcommand{\arraystretch}{1.5}%
	\begin{tabular}{|c|c|c|}	
		\hline \textbf{Method} & \textbf{Time per point [$\mathbf{s}$]} & \textbf{MSE [$\bm{\mu}\mathbf{{m}^2}$]} \\
		\hline Numerical & 4.87 & - \\
		\hline Analytical & 4.43 $\times 10^{-5}$ & 8.47 $\times 10^{-1}$ \\
		\hline ANN & 3.10 $\times 10^{-3}$ & 7.65 $\times 10^{-5}$ \\
		\hline
	\end{tabular}
\end{table*}

Figure \ref{ANN Film Thickness Comparisons Speed Sweep} demonstrates the accuracy of the ANN prediction across the speed range from 1~000~-~21~000~$rpm$. Unlike the divergence of the analytical film from the numerical result as entrainment velocities increase, the ANN demonstrates excellent agreement. The MSE across the speed range is 3.18~$\times 10^{-4}~\mu \mathrm{m}^2$. This proves the effectiveness of this solution across a broad range of operating conditions.

\begin{figure}
	\centering
	\includegraphics[width=150mm]{ANNvsAnalyticalvsNumerical_Speedsweep.png}
	\caption{Central film thickness - EHL vs ANN vs analytical at increasing shaft rotational speed.}
	\label{ANN Film Thickness Comparisons Speed Sweep}
\end{figure}

\subsection{Implicit Bearing Film Thickness Predictions}



\begin{figure}
	\centering
	\includegraphics[width=125mm]{CPU_time_vs_shaft_angular_ANN_implicit.png}
	\caption{ANN vs analytical CPU time per simulation.}
	\label{CPU_time_vs_shaft_angular_ANN_implicit}
\end{figure}

\begin{table*}
	\caption{Implicit CPU time comparison: ANN vs analytical}
	\label{CPU time implicit}
	\centering
	\renewcommand{\arraystretch}{1.5}%
	\begin{tabular}{|p{2.1cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}|p{2cm}<{\centering}|p{2.3cm}<{\centering}|}
		\hline 
		\multirow{2}{*}{\textbf{Speed [rpm]}} & \multicolumn{2}{|c|}{\textbf{CPU time per time step [s]}} & 
		\multirow{2}{=}{\centering \textbf{Relative \\ difference}} & 
		\multirow{2}{=}{\centering \textbf{Factor \\ difference}} & 
		\multirow{2}{=}{\centering \textbf{Mean iterations \\ per time step}} \\
		\cline { 2 - 3 } 
		& \textbf{ANN} & \textbf{Analytical} & & & \\
		\hline 
		5 000 & 0.384 & 6.28 $\times 10^{-3}$ & 1.64 $\times 10^{-2}$ & 61.1 & 2.72 \\
		\hline 
		10 000 & 0.399 & 6.24 $\times 10^{-3}$ & 1.56 $\times 10^{-2}$ & 64.0 & 2.97 \\
		\hline 
		15 000 & 0.367 & 7.60 $\times 10^{-3}$ & 2.06 $\times 10^{-2}$ & 48.5 & 3.56 \\
		\hline 
		20 000 & 0.359 & 7.40 $\times 10^{-3}$ & 2.06 $\times 10^{-2}$ & 48.6 & 3.08 \\
		\hline
	\end{tabular}
\end{table*}

\begin{figure}
	\centering
	\includegraphics[width=125mm]{film_thickness_vs_shaft_sweep_ann_vs_analytical.png}
	\caption{ANN vs analytical film thickness at speed intervals from 5~000~-~20~000~$rpm$.}
	\label{film_thickness_vs_shaft_sweep_ann_vs_analytical.png}
\end{figure}

\begin{table*}
	\caption{Implicit central film thickness comparison: ANN vs analytical}
	\label{Film thickness implicit}
	\centering
	\renewcommand{\arraystretch}{1.5}%
	\begin{tabular}{|p{2.1cm}<{\centering}|p{2.1cm}<{\centering}|p{2.1cm}<{\centering}|p{2.1cm}<{\centering}|}
		\hline 
		\multirow{2}{*}{\textbf{Speed [rpm]}} & \multicolumn{2}{|c|}{\textbf{Central film thickness [$\bm{\mu}\mathbf{m}$]}} & 
		\multirow{2}{=}{\centering \textbf{Percentage \\ difference}} \\
		\cline { 2 - 3 } 
		& \textbf{ANN} & \textbf{Analytical} & \\
		\hline 
		5~000 & 3.26 & 4.00 & 20.3 \\
		\hline 
		10~000 & 2.79 & 3.34 & 18.0 \\
		\hline 
		15~000 & 2.21 & 2.59 & 15.7 \\
		\hline 
		20~000 & 1.47 & 1.67 & 13.2 \\
		\hline
	\end{tabular}
\end{table*}

\section{Conclusions}

A tribological ANN has been trained and employed to calculate the central EHL film thickness at a roller-race conjunction. Different ANN structures were evaluated to find the optimum structure for the film thickness estimation. Numerically generated training data was sampled using Latin Hypercube Sampling (LHS), and constrained to realistic contact conditions using the Greenwood regime. This ensured a high quality dataset; essential for high prediction accuracy. The trained ANN was then used to calculate film thickness in a roller bearing using a simple dynamic bearing model under constant load. The ANN was then employed in an FMBD model, where the film thickness was estimated implicitly at each time step of a dynamic simulation and considered in the prevailing contact mechanics. The following conclusions are drawn from this work:

\begin{enumerate}
	\item Each numerical solution to generate a training data point took 5.88~$s$. The single core wall time for 600 and 5~000 training points was 58.8~$min$ and 489~$min$ respectively. The numerical solution database could benefit from explicit parallelisation i.e.. use of multiple computational cores of the CPU. 
	\item 600 training points achieved sufficient coefficient of determination performance ($R^2$ = 0.99791) to accurately predict central film thickness. Whilst 5000 points resulted in an $R^2$ value of 1, training of the network and effort to generate this quantity of training data rendered it computationally inefficient.
	\item An ANN structure with three hidden layers, each containing 14 neurons and using a logistic sigmoid activation function, was found to be optimal for the operating bounds investigated in this study when trained on 2,000 data points.
	\item The analytical equations increasingly deviate from the numerical calculation at high entrainment velocities; up to 20.3~\% at 21~000~rpm for the bearing examined in this study. At the same speed, the ANN had an error of 1.58~\%. Across a speed range from 0~-~21~000~$rpm$, the MSE of the ANN was 3.18~$\times 10^{-4}~\mu \mathrm{m}^2$.
	\item For the bearing case study presented, the mean squared error (MSE) of the ANN film thickness prediction was 7.65~$\times 10^{-5}~\mu \mathrm{m}^2$ when benchmarked against the numerical solution for a fluctuating contact load. This is a significant improvement over the MSE of the analytical model which was 8.47~$\times 10^{-1}~\mu\mathrm{m}^2$.
	\item The ANN was shown to be $\sim$1~570 times faster than the numerical solution with a very small margin of error. The ANN is a factor of $\sim$75 times slower than the analytical equation, but a factor of 11~$\times 10^{4}$ more accurate when comparing MSE performance against the numerical method.
\end{enumerate}

This investigation has proven ANNs to be an accurate and computationally efficient method of calculating EHL film thickness. Despite having no physical understanding of the system, this data-driven solution has proved to be accurate and computationally efficient for the film thickness estimation. This computational efficiency and accuracy lends itself very well to tribological models in FMBD solvers. It must be noted that the ability of the ANN to extrapolate beyond the bounds of the training data set must be addressed. Excessive extrapolation may lead to instability in the dynamic solution. However, by selecting a sufficient design space and robust sampling method such as LHS, this risk can be mitigated.

Roller bearings are only one application of these ANNs. The use cases extend far beyond roller bearings, to key components in automotive, machining and other industrial applications where interactions between contiguous surfaces exist. Many of these machine elements operate within the regions that this ANN was trained to cover, making it a deployable solution across many applications with low computational training effort.



